{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.5.2-py3-none-any.whl (4.2 MB)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-1.6.0-py3-none-any.whl (48 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.22.1-cp39-cp39-win_amd64.whl (14.7 MB)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\miniconda3\\lib\\site-packages (from tensorflow-datasets) (4.61.2)\n",
      "Collecting absl-py\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from tensorflow-datasets) (2.25.1)\n",
      "Requirement already satisfied: six in c:\\programdata\\miniconda3\\lib\\site-packages (from tensorflow-datasets) (1.16.0)\n",
      "Collecting protobuf>=3.12.2\n",
      "  Downloading protobuf-3.19.4-cp39-cp39-win_amd64.whl (895 kB)\n",
      "Collecting termcolor\n",
      "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting promise\n",
      "  Using cached promise-2.3-py3-none-any.whl\n",
      "Collecting dill\n",
      "  Using cached dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (1.26.6)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.54.0-py2.py3-none-any.whl (207 kB)\n",
      "Collecting colorama\n",
      "  Using cached colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Installing collected packages: protobuf, googleapis-common-protos, colorama, absl-py, termcolor, tensorflow-metadata, promise, numpy, dill, tensorflow-datasets\n",
      "Successfully installed absl-py-1.0.0 colorama-0.4.4 dill-0.3.4 googleapis-common-protos-1.54.0 numpy-1.22.1 promise-2.3 protobuf-3.19.4 tensorflow-datasets-4.5.2 tensorflow-metadata-1.6.0 termcolor-1.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pylint 2.10.2 requires isort<6,>=4.2.5, which is not installed.\n",
      "pylint 2.10.2 requires platformdirs>=2.2.0, which is not installed.\n",
      "pylint 2.10.2 requires toml>=0.7.1, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow-datasets\n",
    "# https://keras.io/examples/vision/nnclr/ (aim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SHUFFLE = 5000\n",
    "N_EPOCH = 64\n",
    "\n",
    "\n",
    "unlabeled_images = 100000\n",
    "labeled_images = 5000\n",
    "\n",
    "input_shape = (96, 96, 3)\n",
    "width = 128\n",
    "\n",
    "cont_aug = {\n",
    "    'brightness' : 0.5, \n",
    "    'name' : 'cont_aug', \n",
    "    'scale' : (0.2, 1.0)\n",
    "}\n",
    "\n",
    "class_aug = {\n",
    "    'brightness' : 0.2, \n",
    "    'name' : 'class_aug', \n",
    "    'scale' : (0.5, 1.0)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset():\n",
    "    unlabeled_batch_size = unlabeled_images // N_EPOCH\n",
    "    labeled_batch_size = labeled_images // N_EPOCH\n",
    "    \n",
    "    batch_size = unlabeled_batch_size + labeled_batch_size\n",
    "    \n",
    "    unlabeled_train_dataset = (\n",
    "        tfds.load(\n",
    "            'stl10', split='unlabelled', as_supervised=True, shuffle_files=True \n",
    "        )\n",
    "        .shuffle(N_SHUFFLE)\n",
    "        .batch(unlabeled_batch_size, drop_remainder=True)\n",
    "    )\n",
    "    \n",
    "    labeled_train_dataset = (\n",
    "        tfds.load(\n",
    "            'stl10', split='train', as_supervised=True, shuffle_files=True \n",
    "        )\n",
    "        .shuffle(N_SHUFFLE)\n",
    "        .batch(unlabeled_batch_size, drop_remainder=True)\n",
    "    )\n",
    "    \n",
    "    test_dataset = (\n",
    "        tfds.load(\n",
    "            'stl10', split='test', as_supervised=True\n",
    "        )\n",
    "        .batch(unlabeled_batch_size, drop_remainder=True)\n",
    "        .prefetch(AUTOTUNE)\n",
    "    )\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.zip(\n",
    "        (unlabeled_train_dataset, labeled_train_dataset)\n",
    "    ).prefetch(AUTOTUNE)\n",
    "     \n",
    "    return batch_size, train_dataset, labeled_train_dataset, test_dataset\n",
    "\n",
    "batch_size, train_dataset, labeled_train_dataset, test_dataset = make_dataset() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomResizedCrop(tf.keras.layers.Layer):\n",
    "    def __init__(self, scale, ratio):\n",
    "        super(RandomResizedCrop, self).__init__()\n",
    "        self.scale = scale \n",
    "        self.log_ratio = (tf.math.log(ratio[0], tf.math.log(ratio[1])))\n",
    "        \n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        height = tf.shape(images)[1]\n",
    "        width = tf.shape(images)[2]\n",
    "        \n",
    "        random_scales = tf.random.uniform((batch_size, ), self.scale[0], self.scale[1])\n",
    "        random_ratios = tf.exp(\n",
    "            tf.random.uniform((batch_size, ), self.log_ratio[0], self.log_ratio[1])\n",
    "        )\n",
    "        \n",
    "        # Tensor 값 범위 지정\n",
    "        new_heights = tf.clip_by_value(tf.sqrt(random_scales / random_ratios), 0, 1)\n",
    "        new_widths = tf.clip_by_value(tf.sqrt(random_scales * random_ratios), 0, 1)\n",
    "        \n",
    "        height_offsets = tf.random.uniform((batch_size, ), 0, 1 - new_heights)\n",
    "        weight_offsets = tf.random.uniform((batch_size, ), 0, 1 - new_widths)\n",
    "        \n",
    "        bounding_boxes = tf.stack(\n",
    "            [\n",
    "                height_offsets, \n",
    "                weight_offsets, \n",
    "                height_offsets + new_heights, \n",
    "                weight_offsets + new_widths, \n",
    "            ],\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        images = tf.image.crop_and_resize(\n",
    "            images, bounding_boxes, tf.range(batch_size), (height, width)\n",
    "        )\n",
    "        \n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomBrightness(tf.keras.layers.Layer):\n",
    "    def __init__(self, brightness):\n",
    "        super(RandomBrightness, self).__init__()\n",
    "        self.brightness = brightness\n",
    "        \n",
    "    def blend(self, images_1, images_2, ratios):\n",
    "        return tf.clip_by_value(ratios * images_1 + (1.0 - ratios) * images_2, 0, 1)\n",
    "    \n",
    "    def random_brightness(self, images):\n",
    "        return self.blend(\n",
    "            images, \n",
    "            0,\n",
    "            tf.random.uniform(\n",
    "                (tf.shape(images)[0], 1, 1, 1), 1 - self.brightness, 1 + self.brightness\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    def call(self, images):\n",
    "        images = self.random_brightness(images)\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug(brightness, name, scale):\n",
    "    return tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Input(shape=input_shape), \n",
    "            tf.keras.layers.Rescaling(1 / 255), \n",
    "            tf.keras.layers.RandomFlip('horizontal'), \n",
    "            RandomResizedCrop(scale=scale, ratio=(3 / 4, 4 / 3)), \n",
    "            RandomBrightness(brightness=brightness)\n",
    "            \n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder():\n",
    "    return tf.keras.Sequentail([\n",
    "        tf.keras.layers.Conv2D(width, kernel_size=3, strides=2, activation='relu'),\n",
    "        tf.keras.layers.Conv2D(width, kernel_size=3, strides=2, activation='relu'),\n",
    "        tf.keras.layers.Conv2D(width, kernel_size=3, strides=2, activation='relu'),\n",
    "        tf.keras.layers.Conv2D(width, kernel_size=3, strides=2, activation='relu'),\n",
    "        tf.keras.layers.Flatten(), \n",
    "        tf.keras.layers.Dense(width, activation='relu')\n",
    "    ], name='encoder'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNCLR(tf.keras.Model):\n",
    "    def __init__(self, temp, queue_size):\n",
    "        super(NNCLR, self).__init__()\n",
    "        self.prob_accuracy = tf.keras.metrics.SparseCategoricalAccuracy() \n",
    "        self.corr_accuracy = tf.keras.metrics.SparseCategoricalAccuracy() \n",
    "        self.cont_accuracy = tf.keras.metrics.SparseCategoricalAccuracy() \n",
    "        \n",
    "        self.prob_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        \n",
    "        self.cont_aug = aug(**cont_aug)\n",
    "        self.class_aug = aug(**class_aug)\n",
    "        self.encoder = encoder()\n",
    "        self.prediction_head = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=(width, )), \n",
    "            tf.keras.layers.Dense(width, activation='relu'), \n",
    "            tf.keras.layers.Dense(width)\n",
    "        ], name='projection_head')\n",
    "        self.linear_probe = tf.keras.Sequenatial([\n",
    "            tf.keras.layers.Input(shape=(width, )), \n",
    "            tf.keras.layers.Dense(10)\n",
    "        ], name='linear_probe')\n",
    "        self.temp = temp \n",
    "        feature_dims = self.encoder.output_shape[1]\n",
    "        self.featrue_queue = tf.Variable(\n",
    "            tf.math.l2_normalize(\n",
    "                tf.random.normal(shape=(queue_size, feature_dims)), axis=1\n",
    "            ), \n",
    "            trainable=False \n",
    "        )\n",
    "        \n",
    "    def compile(self, cont_optimizer, prob_optimizer, **kwargs):\n",
    "        super(NNCLR, self).compile(**kwargs)\n",
    "        self.cont_optimizer  = cont_optimizer\n",
    "        self.prob_optimizer = prob_optimizer\n",
    "        \n",
    "    def nearest_neighbour(self, projections): \n",
    "        support_similarties = tf.matmul(\n",
    "            projections, \n",
    "            self.featrue_queue,\n",
    "            transpose_b = True\n",
    "        )\n",
    "        nn_projections = tf.gather(\n",
    "            self.featrue_queue, \n",
    "            tf.argmax(support_similarties, axis=1), axis=0\n",
    "        )\n",
    "        return projections + tf.stop_gradient(nn_projections - projections) \n",
    "    \n",
    "    def update_cont_accuracy(self, feature1, feature2):\n",
    "        feature1 = tf.math.l2_normalize(feature1, axis=1) \n",
    "        feature2 = tf.math.l2_normalize(feature2, axis=1) \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ddde1e1c6000e1d3034bec6257a8f68bed92c50866ae12a8382b98f1add9b5c2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
